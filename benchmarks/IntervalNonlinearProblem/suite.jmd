---
title: NonlinearSolve.jl suite of interval root-finding algorithms
author: Fabian Gittins
---

In this benchmark, we will examine how the interval root-finding algorithms
provided in `NonlinearSolve.jl` and `SimpleNonlinearSolve.jl` fare against one another for a selection of
challenging test functions from the literature.

## `Roots.jl` baseline

To give us sensible measure to compare with, we will use the `Roots.jl` package
as a baseline,

```julia
using BenchmarkTools
using Roots
```

and search for the roots of the function

```julia
f(u, p) = u * sin(u) - p;
```

To get a good idea of the performance of the algorithms, we will use a large
number of random `p` values and determine the roots with all of them.
Specifically, we will draw `N = 100_000` random values (which we seed for
reproducibility),

```julia
using Random

Random.seed!(42)

const N = 100_000
ps = 1.5 .* rand(N)

function g!(out, ps, uspan)
    for i in 1:N
        out[i] = find_zero(f, uspan, ps[i])
    end
    out
end;
```

Now, we can run the benchmark for `Roots.jl`:

```julia
out = zeros(N)
uspan = (0.0, 2.0)

@btime g!(out, ps, uspan);
```

However, speed is not the only thing we care about. We also want the algorithms
to be accurate. We will use the mean of the absolute errors to measure the
accuracy,

```julia
println("Mean absolute error: $(mean(abs.(f.(out, ps))))")
```

For simplicity, we will assume the default tolerances of the methods, while
noting that these can be set.

## `NonlinearSolve.jl` algorithms

With the preliminaries out of the way, let's see how the `NonlinearSolve.jl`
solvers perform! We define a (non-allocating) function to benchmark,

```julia
using NonlinearSolve

function h!(out, ps, uspan, alg)
    for i in 1:N
        prob = IntervalNonlinearProblem{false}(IntervalNonlinearFunction{false}(f), uspan, ps[i])
        sol = solve(prob, alg)
        out[i] = sol.u
    end
    out
end;
```

and loop through the methods,

```julia
for alg in (Alefeld, NonlinearSolve.Bisection, Brent, Falsi,
            ITP, Muller, Ridder)
    println("Benchmark of $alg:")
    @btime h!($out, $ps, $uspan, $(alg()))
    println("Mean absolute error: $(mean(abs.(f.(out, ps))))\n")
end
```

Although each method finds the roots with different accuracies, we can see that
all the `NonlinearSolve.jl` algorithms are performant and non-allocating.

## A different function

At this point, we will consider a separate function to solve. We will now
search for the root of

```julia
g(u) = exp(u) - 1e-15;
```

The root of this particular function is analytic and given by
`u = - 15 * log(10)`. Due to the nature of the function, it can be difficult to
numerically resolve the root.

Since we do not adjust the value of `p` here, we will just solve this same
function `N` times. As before, we start with `Roots.jl`,

```julia
function i!(out, uspan)
    for i in 1:N
        out[i] = find_zero(g, uspan)
    end
    out
end

uspan = (-100.0, 0.0)

@btime i!(out, uspan)
println("Mean absolute error: $(mean(abs.(g.(out))))")
```

So, how do the `NonlinearSolve.jl` methods fare?

```julia
g(u, p) = g(u)

function j!(out, uspan, alg)
    N = length(out)
    for i in 1:N
        prob = IntervalNonlinearProblem{false}(IntervalNonlinearFunction{false}(g), uspan)
        sol = solve(prob, alg)
        out[i] = sol.u
    end
    out
end

for alg in (Alefeld, NonlinearSolve.Bisection, Brent, Falsi,
            ITP, Muller, Ridder)
    println("Benchmark of $alg:")
    @btime j!($out, $uspan, $(alg()))
    println("Mean absolute error: $(mean(abs.(g.(out))))\n")
end
```

Again, we see that the `NonlinearSolve.jl` root-finding algorithms are fast.
However, it is notable that some are able to resolve the root more accurately
than others. This is entirely to be expected as some of the algorithms, like
`Bisection`, bracket the root and thus will reliably converge to high accuracy.
Others, like `Muller`, are not bracketing methods, but can be extremely fast.

## Extended Test Suite with Challenging Functions

Now we'll test the algorithms on a comprehensive suite of challenging test functions
commonly used in the interval rootfinding literature. These functions exhibit various
difficulties such as multiple roots, nearly flat regions, discontinuities, and
extreme sensitivity.

```julia
using Statistics

# Define challenging test functions
test_functions = [
    # Function 1: Polynomial with multiple roots  
    (name = "Wilkinson-like polynomial", 
     f = (u, p) -> (u - 1) * (u - 2) * (u - 3) * (u - 4) * (u - 5) - p,
     interval = (0.5, 5.5),
     p_range = (-0.1, 0.1)),
     
    # Function 2: Trigonometric with multiple roots
    (name = "sin(x) - 0.5x",
     f = (u, p) -> sin(u) - 0.5*u - p,
     interval = (-10.0, 10.0), 
     p_range = (-0.5, 0.5)),
     
    # Function 3: Exponential function (sensitive near zero)
    (name = "exp(x) - 1 - x - x²/2",
     f = (u, p) -> exp(u) - 1 - u - u^2/2 - p,
     interval = (-2.0, 2.0),
     p_range = (-0.01, 0.01)),
     
    # Function 4: Rational function with pole
    (name = "1/(x-0.5) - 2",
     f = (u, p) -> 1/(u - 0.5) - 2 - p,
     interval = (0.6, 2.0),
     p_range = (-0.1, 0.1)),
     
    # Function 5: Logarithmic function
    (name = "log(x) - x + 2",
     f = (u, p) -> log(u) - u + 2 - p,
     interval = (0.1, 3.0),
     p_range = (-0.1, 0.1)),
     
    # Function 6: High oscillation function
    (name = "sin(20x) + 0.1x",
     f = (u, p) -> sin(20*u) + 0.1*u - p,
     interval = (-5.0, 5.0),
     p_range = (-0.2, 0.2)),
     
    # Function 7: Function with very flat region
    (name = "x³ - 2x² + x",
     f = (u, p) -> u^3 - 2*u^2 + u - p,
     interval = (-1.0, 2.0),
     p_range = (-0.05, 0.05)),
     
    # Function 8: Bessel-like function
    (name = "x·sin(1/x) - 0.1",
     f = (u, p) -> u * sin(1/u) - 0.1 - p,
     interval = (0.01, 1.0),
     p_range = (-0.02, 0.02)),
]

# Add SimpleNonlinearSolve algorithms  
using SimpleNonlinearSolve

# Combined algorithm list from both packages
all_algorithms = [
    (name = "Alefeld (BNS)", alg = () -> Alefeld(), package = "BracketingNonlinearSolve"),
    (name = "Bisection (BNS)", alg = () -> NonlinearSolve.Bisection(), package = "BracketingNonlinearSolve"), 
    (name = "Brent (BNS)", alg = () -> Brent(), package = "BracketingNonlinearSolve"),
    (name = "Falsi (BNS)", alg = () -> Falsi(), package = "BracketingNonlinearSolve"),
    (name = "ITP (BNS)", alg = () -> ITP(), package = "BracketingNonlinearSolve"),
    (name = "Ridder (BNS)", alg = () -> Ridder(), package = "BracketingNonlinearSolve"),
    (name = "Bisection (SNS)", alg = () -> SimpleNonlinearSolve.Bisection(), package = "SimpleNonlinearSolve"),
    (name = "Brent (SNS)", alg = () -> SimpleNonlinearSolve.Brent(), package = "SimpleNonlinearSolve"),
    (name = "Falsi (SNS)", alg = () -> SimpleNonlinearSolve.Falsi(), package = "SimpleNonlinearSolve"),
    (name = "Ridders (SNS)", alg = () -> SimpleNonlinearSolve.Ridders(), package = "SimpleNonlinearSolve")
]

# Benchmark function for testing all algorithms on a given function
function benchmark_function(test_func, N_samples=10000)
    Random.seed!(42)
    ps = test_func.p_range[1] .+ (test_func.p_range[2] - test_func.p_range[1]) .* rand(N_samples)
    
    println("\\n=== Testing: $(test_func.name) ===")
    println("Interval: $(test_func.interval)")
    println("Parameter range: $(test_func.p_range)")
    
    results = []
    
    # Test Roots.jl baseline
    try
        out_roots = zeros(N_samples)
        time_roots = @elapsed begin
            for i in 1:N_samples
                out_roots[i] = find_zero(u -> test_func.f(u, ps[i]), test_func.interval)
            end
        end
        error_roots = mean(abs.(test_func.f.(out_roots, ps)))
        println("Roots.jl: $(round(time_roots*1000, digits=2)) ms, MAE: $(round(error_roots, sigdigits=3))")
        push!(results, (name="Roots.jl", time=time_roots, error=error_roots, success=true))
    catch e
        println("Roots.jl: FAILED - $e")
        push!(results, (name="Roots.jl", time=Inf, error=Inf, success=false))
    end
    
    # Test all algorithms
    for alg_info in all_algorithms
        try
            out = zeros(N_samples)
            time_taken = @elapsed begin
                for i in 1:N_samples
                    prob = IntervalNonlinearProblem{false}(
                        IntervalNonlinearFunction{false}((u, p) -> test_func.f(u, p)), 
                        test_func.interval, ps[i])
                    sol = solve(prob, alg_info.alg())
                    out[i] = sol.u
                end
            end
            error_val = mean(abs.(test_func.f.(out, ps)))
            println("$(alg_info.name): $(round(time_taken*1000, digits=2)) ms, MAE: $(round(error_val, sigdigits=3))")
            push!(results, (name=alg_info.name, time=time_taken, error=error_val, success=true))
        catch e
            println("$(alg_info.name): FAILED - $e")
            push!(results, (name=alg_info.name, time=Inf, error=Inf, success=false))
        end
    end
    
    return results
end

# Run benchmarks on all test functions
all_results = []
for test_func in test_functions
    results = benchmark_function(test_func, 5000)  # Use smaller N for comprehensive testing
    push!(all_results, (func_name=test_func.name, results=results))
end
```

## Performance Summary

Let's create a summary table of the results:

```julia
using Printf

function print_summary_table(all_results)
    println("\\n" * "="^80)
    println("COMPREHENSIVE BENCHMARK SUMMARY")
    println("="^80)
    
    # Get all algorithm names
    alg_names = unique([r.name for func_results in all_results for r in func_results.results])
    
    # Print header
    @printf "%-25s" "Function"
    for alg in alg_names
        @printf "%-15s" alg[1:min(14, length(alg))]
    end
    println()
    println("-"^(25 + 15*length(alg_names)))
    
    # Print results for each function
    for func_result in all_results
        @printf "%-25s" func_result.func_name[1:min(24, length(func_result.func_name))]
        
        for alg in alg_names
            # Find result for this algorithm
            alg_result = findfirst(r -> r.name == alg, func_result.results)
            if alg_result !== nothing
                result = func_result.results[alg_result]
                if result.success && result.time < 1.0  # Reasonable time limit
                    @printf "%-15s" "$(round(result.time*1000, digits=1))ms"
                else
                    @printf "%-15s" "FAIL"
                end
            else
                @printf "%-15s" "N/A"
            end
        end
        println()
    end
    
    println("\\n" * "="^80)
    println("Notes:")
    println("- Times shown in milliseconds for 5000 function evaluations") 
    println("- BNS = BracketingNonlinearSolve.jl, SNS = SimpleNonlinearSolve.jl")
    println("- FAIL indicates algorithm failed or took excessive time")
    println("="^80)
end

print_summary_table(all_results)
```

## Accuracy Analysis

Now let's examine the accuracy of each method:

```julia
function print_accuracy_table(all_results)
    println("\\n" * "="^80)
    println("ACCURACY ANALYSIS (Mean Absolute Error)")
    println("="^80)
    
    alg_names = unique([r.name for func_results in all_results for r in func_results.results])
    
    # Print header
    @printf "%-25s" "Function"
    for alg in alg_names
        @printf "%-15s" alg[1:min(14, length(alg))]
    end
    println()
    println("-"^(25 + 15*length(alg_names)))
    
    # Print results for each function
    for func_result in all_results
        @printf "%-25s" func_result.func_name[1:min(24, length(func_result.func_name))]
        
        for alg in alg_names
            alg_result = findfirst(r -> r.name == alg, func_result.results)
            if alg_result !== nothing
                result = func_result.results[alg_result]
                if result.success && result.error < 1e10
                    @printf "%-15s" "$(round(result.error, sigdigits=2))"
                else
                    @printf "%-15s" "FAIL"
                end
            else
                @printf "%-15s" "N/A"
            end
        end
        println()
    end
    
    println("="^80)
end

print_accuracy_table(all_results)
```

## Algorithm Rankings

Finally, let's rank the algorithms by overall performance:

```julia
function rank_algorithms(all_results)
    println("\\n" * "="^60)
    println("ALGORITHM RANKINGS")
    println("="^60)
    
    # Calculate scores for each algorithm
    alg_scores = Dict()
    
    for func_result in all_results
        for result in func_result.results
            if !haskey(alg_scores, result.name)
                alg_scores[result.name] = Dict(:time_score => 0.0, :accuracy_score => 0.0, :success_count => 0)
            end
            
            if result.success
                alg_scores[result.name][:success_count] += 1
                # Lower time is better (inverse score)
                alg_scores[result.name][:time_score] += result.time < 1.0 ? 1.0 / result.time : 0.0
                # Lower error is better (inverse score) 
                alg_scores[result.name][:accuracy_score] += result.error < 1e10 ? 1.0 / (result.error + 1e-15) : 0.0
            end
        end
    end
    
    # Normalize and combine scores
    total_functions = length(all_results)
    algorithm_rankings = []
    
    for (alg, scores) in alg_scores
        success_rate = scores[:success_count] / total_functions
        avg_speed_score = scores[:time_score] / total_functions
        avg_accuracy_score = scores[:accuracy_score] / total_functions
        
        # Combined score (weighted: 40% success rate, 30% speed, 30% accuracy)
        combined_score = 0.4 * success_rate + 0.3 * (avg_speed_score / 1000) + 0.3 * (avg_accuracy_score / 1e12)
        
        push!(algorithm_rankings, (
            name = alg,
            success_rate = success_rate,
            speed_score = avg_speed_score,
            accuracy_score = avg_accuracy_score,
            combined_score = combined_score
        ))
    end
    
    # Sort by combined score
    sort!(algorithm_rankings, by = x -> x.combined_score, rev = true)
    
    println("Rank | Algorithm          | Success Rate | Combined Score")
    println("-"^60)
    for (i, alg) in enumerate(algorithm_rankings)
        @printf "%-4d | %-18s | %-11.1f%% | %-12.3f\\n" i alg.name[1:min(18, length(alg.name))] (alg.success_rate*100) alg.combined_score
    end
    
    println("="^60)
    println("Note: Combined score weights success rate (40%), speed (30%), and accuracy (30%)")
end

rank_algorithms(all_results)
```

## Conclusion

This extended benchmark suite demonstrates the performance and accuracy characteristics of interval rootfinding algorithms across a diverse set of challenging test functions. The test functions include:

1. **Polynomial functions** with multiple roots
2. **Trigonometric functions** with oscillatory behavior  
3. **Exponential functions** with high sensitivity
4. **Rational functions** with singularities
5. **Logarithmic functions** with domain restrictions
6. **Highly oscillatory functions** testing robustness
7. **Functions with flat regions** challenging convergence
8. **Bessel-like functions** with complex behavior

The benchmark compares algorithms from both `BracketingNonlinearSolve.jl` and `SimpleNonlinearSolve.jl`, providing insights into:
- **Robustness**: Which algorithms handle challenging functions
- **Speed**: Computational efficiency across different problem types
- **Accuracy**: Precision of the found roots
- **Reliability**: Success rates across diverse test cases

This comprehensive evaluation helps users choose the most appropriate interval rootfinding algorithm for their specific applications.

```julia, echo = false
using SciMLBenchmarks
SciMLBenchmarks.bench_footer(WEAVE_ARGS[:folder], WEAVE_ARGS[:file])
```
